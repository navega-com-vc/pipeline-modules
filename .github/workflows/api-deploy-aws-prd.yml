name: api-deploy-prd

on:
  workflow_call:
    inputs:
      WORKING_DIRECTORY:
        type: string
      RUNS_ON:
        type: string
        default: ubuntu-latest

    secrets:
      ARTIFACTORY_NPM_REGISTRY:
        required: true
      ARTIFACTORY_AUTH:
        required: true
      ARTIFACTORY_EMAIL:
        required: true

      AWS_ACCESS_KEY_ID_PRD:
        required: true
      AWS_SECRET_ACCESS_KEY_PRD:
        required: true
      AWS_REGION_PRD:
        required: true

      SUBNET_APP_A_PRD:
        required: true
      SUBNET_APP_B_PRD:
        required: true
      SECURITY_GROUP_API_PRD:
        required: true

      ENV_PRD:
        required: true
      LOG_LEVEL_PRD:
        required: true
      DD_API_KEY:
        required: true

      NAVEGA_API_URL_PRD:
        required: false

      DATABASE_HOST_PRD:
        required: false
      DATABASE_USERNAME_PRD:
        required: false
      DATABASE_PASSWORD_PRD:
        required: false
      DATABASE_NAME_PRD:
        required: false

      KEYCLOAK_URL_PRD:
        required: false
      KEYCLOAK_USERNAME_ADMIN_PRD:
        required: false
      KEYCLOAK_PASSWORD_ADMIN_PRD:
        required: false

      AWS_DYNAMODB_REGION_PRD:
        required: false
      AWS_DYNAMODB_VERSION_PRD:
        required: false
      AWS_DYNAMODB_MIRRORED_DOCUMENTS_TABLE_PRD:
        required: false

      USE_WRITE_COMMAND_WITH_QUEUE_PRD:
        required: false
      WRITE_COMMAND_MESSAGE_BATCH_SIZE_PRD:
        required: false

      # env MAX_OUTPUT_DATA_SIZE_IN_MB utilizada no workflows-api, para limitar o tamanho do output maximo a ser retornado
      # limitamos ele em uma tamanho menor, pois a ideia é q o output limitado, seja utilizado mais pelo front
      # dessa maneira o tamanho do output é limitado a 1MB, q ja é bastante informação para ser exibida
      MAX_OUTPUT_DATA_SIZE_IN_MB:
        required: false

      # env RULES_API_MAX_OUTPUT_DATA_SIZE_IN_MB é similar ao MAX_OUTPUT_DATA_SIZE_IN_MB, mas quem utilizará esse endpoint
      # serão os backend, com os resultados dos processos, por isso o tamanho do output é maior
      RULES_API_MAX_OUTPUT_DATA_SIZE_IN_MB:
        required: false
      TIMEOUT_TASK_EXECUTION_IN_SECONDS:
        required: false
      FREE_MEMORY_LIMIT_IN_MB:
        required: false

      MFA_KEY_PRD:
        required: false

jobs:
  deploy-prd:
    if: github.ref == 'refs/heads/main'
    runs-on: ${{ inputs.RUNS_ON }}
    concurrency:
      group: ${{ github.workflow }}-${{ github.ref }}
      cancel-in-progress: true

    env:
      DATABASE_HOST: ${{secrets.DATABASE_HOST_PRD}}
      DATABASE_USERNAME: ${{secrets.DATABASE_USERNAME_PRD}}
      DATABASE_PASSWORD: ${{secrets.DATABASE_PASSWORD_PRD}}
      DATABASE_NAME: ${{secrets.DATABASE_NAME_PRD}}

      DATABASE_DATA_SOURCE: ${{ secrets.DATABASE_DATA_SOURCE_PRD }}
      DATABASE_API_URL: ${{ secrets.DATABASE_API_URL }}
      DATABASE_API_KEY: ${{ secrets.DATABASE_API_KEY_PRD }}

      KEYCLOAK_URL: ${{secrets.KEYCLOAK_URL_PRD}}
      KEYCLOAK_USERNAME_ADMIN: ${{secrets.KEYCLOAK_USERNAME_ADMIN_PRD}}
      KEYCLOAK_PASSWORD_ADMIN: ${{secrets.KEYCLOAK_PASSWORD_ADMIN_PRD}}

      SUBNET_APP_A: ${{secrets.SUBNET_APP_A_PRD}}
      SUBNET_APP_B: ${{secrets.SUBNET_APP_B_PRD}}
      SECURITY_GROUP_API: ${{secrets.SECURITY_GROUP_API_PRD}}

      LOG_LEVEL: ${{secrets.LOG_LEVEL_PRD}}
      DD_API_KEY: ${{secrets.DD_API_KEY}}
      ENV: ${{secrets.ENV_PRD}}

      NAVEGA_API_URL: ${{secrets.NAVEGA_API_URL_PRD}}

      AWS_DYNAMODB_REGION: ${{secrets.AWS_REGION_PRD}}
      AWS_DYNAMODB_VERSION: ${{secrets.AWS_DYNAMODB_VERSION_PRD}}
      AWS_DYNAMODB_MIRRORED_DOCUMENTS_TABLE: ${{secrets.AWS_DYNAMODB_MIRRORED_DOCUMENTS_TABLE_PRD}}

      EMAIL_PROVIDER: ${{ secrets.EMAIL_PROVIDER_PRD }}
      EMAIL_USERNAME: ${{ secrets.EMAIL_USERNAME_PRD }}
      EMAIL_PASSWORD: ${{ secrets.EMAIL_PASSWORD_PRD }}

      USE_WRITE_COMMAND_WITH_QUEUE: ${{ secrets.USE_WRITE_COMMAND_WITH_QUEUE_PRD }}
      WRITE_COMMAND_MESSAGE_BATCH_SIZE: ${{ secrets.WRITE_COMMAND_MESSAGE_BATCH_SIZE_PRD }}

      USE_ATLAS_API_REQUEST: ${{ secrets.USE_ATLAS_API_REQUEST }}

      MAX_OUTPUT_DATA_SIZE_IN_MB: ${{ secrets.MAX_OUTPUT_DATA_SIZE_IN_MB }}
      RULES_API_MAX_OUTPUT_DATA_SIZE_IN_MB: ${{ secrets.RULES_API_MAX_OUTPUT_DATA_SIZE_IN_MB }}
      TIMEOUT_TASK_EXECUTION_IN_SECONDS: ${{ secrets.TIMEOUT_TASK_EXECUTION_IN_SECONDS}}
      FREE_MEMORY_LIMIT_IN_MB: ${{ secrets.FREE_MEMORY_LIMIT_IN_MB}}

      MFA_KEY: ${{ secrets.MFA_KEY_PRD }}
    steps:
      - name: Checkout
        uses: actions/checkout@v3
        with:
          ref: ${{ github.ref }}

      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: 20

      - name: Install
        working-directory: ${{ inputs.WORKING_DIRECTORY }}
        run: |
          npm config set registry ${{secrets.ARTIFACTORY_NPM_REGISTRY}}
          npm config set -- '//f3capital.jfrog.io/artifactory/api/npm/f3capital-npm/:_auth' "${{secrets.ARTIFACTORY_AUTH}}"
          npm config set -- 'email' "${{secrets.ARTIFACTORY_EMAIL}}"
          echo "always-auth=true" >> ~/.npmrc
          
          # Install dependencies in the root directory
          if [ -f "package.json" ]; then
            yarn install
          fi

          # Install dependencies in subdirectories of 'layer'
          if [ -d "layer" ]; then
            for dir in layer/*/; do
              if [ -f "$dir/package.json" ]; then
                (cd "$dir" && yarn install)
              fi
            done
          fi

      - name: Production Deploy
        working-directory: ${{ inputs.WORKING_DIRECTORY }}
        run: |
          export VERSION=$(node -p "require('./package.json').version")
          export AWS_ACCESS_KEY_ID=${{secrets.AWS_ACCESS_KEY_ID_PRD}}
          export AWS_SECRET_ACCESS_KEY=${{secrets.AWS_SECRET_ACCESS_KEY_PRD}}
          export AWS_REGION=${{secrets.AWS_REGION_PRD}}
          npm i -g serverless@3
          sls deploy --stage ${{secrets.ENV_PRD}} --region $AWS_REGION
